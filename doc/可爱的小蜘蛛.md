[toc]
*** 

# 可爱的小蜘蛛
## 编写爬虫的一般流程：
   查看Headers选项卡，查看请求需要用到的请求头、请求参数等。然后查看Response 返回的网页结构，在看要解析的节点，有时也可以直接查看Elements选项卡，但是对于JavaScript 动态生成的网页，还是得查看Response选项卡返回的内容。
## 常用与易错
### 常用
+ str.join(list1)//以str为中间字符将list1中的元素连接起来。
+ data = [str(item)+'\n' for item in items]//将结果转换为字符串，并添加换行符。

### 易错
+ 字典的修改：~~dict[name]="ksdlf"~~,应为dict["name"]=ksdlf. 


## 辅助开发工具：
### Chrome浏览器
+ console(控制台)：查看网页源代码，查看请求头，查看响应头，查看cookie，输入JavaScript代码执行Console可测试JavaScript代码,也可测试css选择器。
### Fiddler
+ 以代理服务器的方式监听网络请求，可以抓包，修改请求头，修改响应头，修改响应内容。
+ 
  - 它使用代理地址：127.0.0.1:8888，在浏览器中设置代理，在Fiddler中设置规则，在浏览器中访问网页，Fiddler就会抓包。端口为8888，可以通过跳过HOST来扩大或缩小抓包范围。
  - Host切换：场景模拟
    - 通过GZIP压缩，测试性能。
    - 模拟agent测试查看服务器是响应是否对浏览器敏感。
    - 模拟慢网速，测试页面的容错性。(Rules->Performance->Simulate Modem Speeds)
    - 禁用缓存，方便测试静态文件或测试服务端响应情况。 
+ Composer(构造器)
  - 可不改变代码下，修改请求参数。
+ Filters(过滤器)
  - 过滤器，过滤请求或响应,保证只得到需要的包。
+ AutoResolve(自动重定向)
  - 将服务器的资源复制到本地，通过将请求重定向到本地副本，可无干扰开发。
+ 移动端抓包
  - 手机浏览器安装Fiddler插件，在手机上安装Fiddler证书，手机浏览器设置代理，设备需要在同一局域网。
  - 在Fiddler设置中打开fiddler,单击Tool->Fidler Options->重启
  - 选中Allow Unauthorized Certificates,允许别的设备发送请求到Fidder上。


## VirtualEnv虚拟环境
+ 目的：让Python项目具有独立的环境互相不干扰，Python2和3可共存。
+ 安装：pip install virtualenv
+ 初始化与激活： 
## Http网页请求方式：
+ http的工作原理：
  - 客户端与服务器建立TCP连接，客户端向服务器发送请求报文，服务器接收请求并返回响应。
  - 服务器接收到的请求报文，返回响应报文给客户端。解析请求定位资源，将资源副本写的TCP端由客户端读取。
  - 释放TCP连接，若connection为keep-alive，则服务器保持TCP连接一段时间，等待客户端再次发送请求。若为close，则关闭连接。
+ 请求报文：
  - 请求行：请求方法，请求资源路径，请求版本。
  - 请求头：键值对，键值对之间用空格分隔。最后一个是空行，用来分割请求头和请求体。
  - 请求体：POST请求时，请求体中包含请求参数。
  - 请求报文示例：
    ```
    GET / HTTP/1.1
    Host: www.baidu.com
    Connection: keep-alive
    Upgrade-Insecure-Requests: 1
    User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36
    Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp，
    Accept-Encoding: gzip, deflate, br
    Accept-Language: zh-CN,zh;q=0.9
    ```
- 状态码：
  - 1XX：服务器收到请求，需要请求者继续执行操作。
  - 2XX：服务器成功处理请求，并返回响应。
  - 3XX：重定向，需要进一步的操作以完成请求。
  - 4XX：客户端错误，请求包含语法错误或无法完成请求。
  - 5XX：服务器错误，服务器在处理请求的过程中发生了错误。
  - 常见状态码：
    - 200：请求成功。
    - 400：请求报文有语法错误。
    - 401：未授权。
    - 403：服务器拒绝请求。
    - 404：服务器找不到请求的资源。
    - 500：服务器内部错误。
    - 503：服务器繁忙。
+ 常见http请求方式
  - GET：请求获取URL中的资源，请求参数在URL中，只需一次发送和返回响应速度快。
  - POST：请求获取URL中的资源，请求参数在请求体中，需两次发送和返回响应速度慢。
  - PUT：请求获取URL中的资源，请求参数在请求体中，需两次发送和返回响应速度慢。类似post，但是更新数据。
  - DELETE：删除给定位置的信息。
  - HEAD：请求获取响应的报文，不返回响应体。
  - OPTIONS：获取URL支持的请求方式。
- GET（可用url传递参数），获取查询信息，参数在URL中，只需一次发送和返回响应速度快。
对应抓取方式：import requests #导入requests库
url='http://www.******.cn/"  #网址，URL分为基础部分不可变和参数部分可变。-
strhtml=requests.get(url)  # 按GET方式利用requests.get（）函数获取网页数据，strhtml变量此时是一个URL对象代表整个网页，strhtml.txt表示网页源码。
print(strhtml.text)  #输出
+ 参数化页面
  - 查询字符串
   URL中域名带？表示URL带有请求参数后有参数名=number，是较为古老的方式。
   ```
    http://www.runoob.com/s?wd=python3可自己设定，如：
     url=http://www.runoob.com/s
    params={"wd":"python"}
    r=requests.get(url,params=params)#动态设置。
    URL = 'http://www.cxbz958.org/%s/%d.html' % (num,str1)可动态设置URL。
    ```
  - 路由生成：
    由路由生成URL，通过字典生成URL，字典的键是参数名，值是参数值。
    像是静态文件，更受搜索引擎的喜欢。
- POST，参数（date=）在request body(实体) 中，可发送的请求信息多（字典，元组，列表，JOSN等）。
开发者模式->Network选项卡->“XHR”(相关数据)->单击Headers（（requestas Method）找到数据请求方式）,j将Headers中的request URL复制并且赋给url,
- POST参数在实体中，要将请求的参数fromdate放在一个新字典(如data)中，Response = requests.post(url,date=from_data).
- Import json
Content = json.loads(response.text)#将josn格式的字符串转换为字典。
## 4.	网页解析
### **CSS选择器**
+	安装bs4库（因为BeautfulSoup已经内置在bs4中了，导入时from bs4 import BeautfulSoup），安装lxml库（Python内置了HTML库但是lxml库功能强大。）
+ BeautfulSoup解析数据：suop = BeautfulSoup(strhtml.txt,’lxml’)#使用lxml库解析数据，将解析后的数据保存在新变量soup中。
+ 数据定位：
 - 找到对应路径：右击网页需要对应的数据->检查->右击高亮的对应代码->选择copy（复制） ->copy selector(复制路径)
 - 使用 soup.select: data=soup.select(‘路径’)
+ 数据定位后得到了目标HTML代码,要马上保存原始数据，后可以for循环提取数据
```
   For item in data  :
	Result={
		‘title’:item.get_text() 提取标题。标题在<"a">标签中，提取正文用get_text()
		‘link’:item.get(‘href’) 提取链接，提取标签中的href属性用get()方法。
           } 
```
#### CSS语法
+ CSS选择器：右击需要提取的节点->选择Copy->Copy selector(复制选择器)
+ CSS语法：CSS选择器 = 选择器类型 [属性] [* | 属性 | 文本]
+ 常见的实例
  - 提取所有的链接：a
  - 提取所有的文本：#text
  - 提取所有的class属性：*[@class]
  - 提取具有特定类名的链接：a[@class="title"]
  - 提取具有特定文本的链接：a[text(),"example"]
+ 一次提取类似路径
  - 常用的伪选择器：
    * ::text //提取元素的文本。
    * ::attr(attribute) //提取属性为attribute的元素。
    * ::class //提取class为class的元素。
    * ::link //提取元素的链接
    * ::img //提取img元素
  - div.container div.item::text //提取div.container div.item下的所有文本
### **re 正则提取**
#### 一次提取所有符合内容
+ 使用find_all()方法，参数为正则表达式，返回所有符合的内容。
+ 正则表达式通常要先匹配一个空白字符。

#### 语法
+ 正则表达式：每一篇文章都有一个数字ID可用正则表达式提取，**以单引号括起来**。
 - 正则表达式符号：\d匹配数字 +匹配前一个数字一次或多次。
  * 单一个点，匹配除‘\n’外任意一个字符。
  *  $匹配$之前的字符或模式结束的字符。
  *  \f匹配换页符，\n匹配换行符,\r匹配回车符。
  * \d<=>[0-9]匹配一个数字，\D<=>[^0-9]匹配一个非数字
  * \s匹配空白字符， \S匹配非空白字符
  * \w<=>[A-Za-z0-9]匹配任意单词字符，\W匹配任意非单词字符。eg：'\w\w\d'可匹配‘py3’
  * ^匹配开始位置，多行模式下匹配到每一行开始。$匹配结束位置，多行模式下匹配到每一行结束
  *  **\^表示开头(^\d表示必须以数字开头)，$表示结尾(\d\$表示必须与数字结尾)。|表示或(A|a匹配A,a),后接{1,19}则是限制了变量长度为1到19，正常是贪婪匹配（匹配尽可能多），末尾加？可变为非贪婪匹配** ***后接加号匹配至少由一个数字、字母、下划线组成的字符串，后接乘号匹配由字母或下化线开头后接任意个数字组成的字符串***
 - Eg：‘ID’:re.findall(‘\d+’,item.get(‘href’))//fingdall(正则表达式,提取的文本)
 - {a}匹配前一个字符a次。{m,n}匹配前一个字符m到n次 {m,n}?匹配前一个字符m到n次取尽可能少的情况
 - \\对特殊字符转义，[abc]字符集
 - (?=abc)从左至右匹配到abc停止保留abc前面的部分，(?!=abc)与前者相反。
+ 可选标志修饰符：
  | 标志 | 描述 |
  | :-: | :-: |
  | re.I | 使匹配对大小写不敏感 |
  | re.L | 做本地化识别(locale-aware)匹配 |
  | re.M | 多行匹配，影响^和$ |
  | re.S | 使.匹配包括换行在内的所有字符 |
  | re.U | 根据Unicode字符集解析字符。这个标志影响\w, \W, \b, \B. |
  | re.X | 该标志通过给予你更灵活的格式使正则表达式更易读。 |
### **xpath提取**
#### 格式
+ sel = Selector(response) //对response进行封装，让其可以调用sel.xpath(xpath)方法
+ data = sel.xpath("//h1")
#### xpath语法
+ xpath提取：右击需要提取的节点->选择copy->copy xpath(复制路径)
+ XPath表达式 = 路径表达式 [@属性] [文本] [* | 属性 | 文本],路径表达式用于指定路径，@属性用于指定属性，文本用于指定文本，*为贪婪匹配，@表示匹配属性，**提取返回列表。**
+ 
  - **xpath返回是列表**，list的成员类型为selector,**使用extract提取后**为列表成员类型为selector中data的字符串,同样css也可使用，**返回的还是列表即使仅有一个元素，如直接使用了urljoin方法会报错因类型不匹配。**
  - extract_first() 方法：**只返回匹配到的第一个元素的文本内容**，在实际应用中，经常会使用 extract_first()，因为它简便，而且当你知道只有一个匹配时，它更容易处理。
+ xpath常用的选择器：
  - // 匹配文档中的所有元素
  - / 匹配文档中的子元素
  - . 匹配当前元素
  - .. 匹配当前元素的父元素
  - @ 匹配属性
+ 常见的实例
  - 提取所有的链接：//a/@href
  - 提取所有的文本：//text()
  - 提取所有的class属性：//[@class]
  - 提取具有特定类名的链接：//[@class="title"]/@href
  - 提取具有特定文本的链接：//a[text()="Python"]/@href
  - response.xpath('//titile/text()').extract()
+ 常见的易错实例
  - //node[1] : 匹配所有名为node的第一个子节点
  - (//node)[1] : 匹配第一个名为node子节点。
+ 轴::标签
  - 轴
    | 轴名称 | 结果 |
    | :-: | :-: |
    | ancestor | 选取当前节点的所有先辈（父、祖父等） |
    | ancestor-or-self | 选取当前节点的所有先辈（父、祖父等）以及当前节点本身 |
    | attribute | 选取当前节点的所有属性 |
    | child | 选取当前节点的所有子节点 |
    | descendant | 选取当前节点的所有后代节点（子、孙等） |
    | descendant-or-self | 选取当前节点的所有后代节点以及当前节点本身 |
    | following | 选取文档中当前节点的结束标签之后的所有节点 |
    | following-sibling | 选取当前节点之后的所有兄弟节点 |
    | namespace | 选取当前节点的所有命名空间节点 |
    | parent | 选取当前节点的父节点 |
    | preceding | 选取文档中当前节点的开始标签之前的所有节点 |
    |self|当前节点|
  - 标签
    | 标签 | 结果 |
    | :-: | :-: |
    | div |所有名为div的节点|
    |text（）|文本节点|
### 常用匹配函数
  * search(pattern正则表达式，str，flags标志位)//标志位通常写为**re.I****(I(忽略大小写) M(开启多行模式) S(点可匹配换行符) L(字符集本地化) U(使预定字符集（如\d）取决于Unicode定义的字符属性 ) X(详细模式，正则表达式可以是多行的忽略空白字符并可加入注释) M|I表示两个都要 )**，匹配好后用group(N)获得第N个正则表达式的值无参数为所有的正则表达式值。
  * **match**同上，但是search匹配整个字符串直到找到，match如果开头不匹配则匹配失败。
  * **findall**参数同上，但返回是列表。1.当正则表达式有多个括号，返回的列表元素为多个字符串组成的元组。2.只有一个括号，返回列表元素为字符串。3.无括号返回为整个正则表达式匹配的内容。
  * **re.sub(pattern正则表达式，repl用于替换的字符串，str被替换的字符串，count替换的次数，flags标志位同上)**

### **scrapy数据的提取**
+ 在scrapy框架中得到的response可以直接使用response.xpath()函数，返回一个列表，列表中是xpath表达式，或response.css()函数，返回一个列表，列表中是css表达式。
+ response.xpath('//titile/text()').extract() //**xpath返回是列表，list的成员类型为selector,使用extract提取后为列表成员类型为selector中data的字符串,同样css也可使用**
+ 嵌套（链式）选择器：选择器是可以嵌套使用的，例如：
```
response.xpath('//item').css('title::text').extract()
```
+ 结合re表达式使用选择器：
  re()方法，返回字符串列表，不可嵌套。
+ **在scrapy中常常使用css选择类，具体在用Xpath,因scrapy支持链式选择**
#### scrapy非结构化数据提取
+ 非结构化数据：没有固定格式的数据，如html,xml,json,txt,csv等。
+ 非结构化数据提取：
  - 非结构化数据提取：
    - 文本文件的提取：正则表达式，
#### 处理页面动态加载(JavaScript)
+ 处理动态加载：
  - Selenium + WebDriver
   - Selenium为浏览器驱动代理，需配合本地浏览器使用，基于Java，使用需要Java命令可用，若不可用需要安装Java环境。
   - PhantomJS:
     - 基于Webkit的JavaScript无界面浏览器，可完全模拟浏览器进行网页加载.浏览器打开一个页面需要200MB左右内存，当多线程时内存消耗大无法承受。
    ```
  from selenium import webdriver

  browser = webdriver.PhantomJS() #加载PhantomJS浏览器
  browser.get(''http://www.xxx.com") #打开网址
  broweser.implicitly_wait(10) #设置等待时间
  print browser.get_cookies() #打印cookies
    ```
  - Splash
+ 
##### selenium使用
+ 安装：
  - pip install selenium
+ 基本使用：
```
from selenium import webdriver
from selenium.webdriver.common.keys import Keys

browser = webdriver.Chrome() #创建Firefox浏览器对象
diver.get("http://www.python.com") #打开网址
assert "Python" in browser.title #断言标题包含Python  
elem = driver.find_element_by_name("q") #定位元素
elem.send_keys("pycon") #输入内容
elem.send_keys(Keys.RETURN) #回车
assert "No results found." not in browser.page_source #断言页面源码不包含No results found.
browser.close() #关闭浏览器
```
 - 关闭网页：quit()退出浏览器，close()关闭当前窗口。
 - 查找网页元素：一个用element多个用elements返回一个list 
   - find_element_by_id()：通过id查找
   - find_element_by_name()：通过name查找
   - find_element_by_xpath()：通过xpath查找
   - find_element_by_css_selector(): 通过css查找
   - find_element_by_link_text(): 查找具有特定文本的链接
   - find_element_by_partial_link_text(): 查找包含部分指定文本的链接
   - find_element_by_tag_name(): 通过标签名查找
   - find_element_by_class_name(): 通过class查找 
 - 文本输入：send_keys()，
  - send_keys(Keys.RETURN)（特殊字符可表示特殊键，此为输入回车）
  - element.send_keys(" and some",Keys.RETURN)(模拟方向键)
- 填充表单：
  -
  ```
  from selenium.webdriver.support.ui import Select
  select = Select(driver.find_element_by_name('name')) #选中下拉框
  select.selct_by_index(index) #选中下拉框的索引
  select.select_by_visible_text(text) #选中下拉框的文本
  select.select_by_value(value) #选中下拉框的值

  select = Select(driver.find_element_by_id('id')) #选中下拉框
  select.deselect_all() #取消所有选中
  select.deselect(index) # 取消选中下拉框的索引

  select = Select(driver.find_element_by_xpath('xpath')) #选中下拉框
  all_select_options = select.all_selected_options #获取所有选中项
  options = select.options #获取所有选项

  # submit提交表达方法，找到submit按钮单击。
  driver.find_element_by_id("submit").click() #点击按钮
  ```

+
鼠标事件：
|方法|说明|参数说明|
|:---|:---|:---|
|click(on_element=None)|单击|None为单击的元素，空为当前|
|click_and_hold(on_element=None)|单击按住|None为按住元素的元素，空为当前|
|double_click(on_element=None)|双击|None为双击的元素，空为当前|
|drag_and_drop(source, target)|拖动|source为源元素，target为目标元素|
|drag_and_drop_by_offset(source, xoffset, yoffset)|拖动|source为源元素，xoffset为x轴偏移，yoffset为y轴偏移|
|key_down(value, element=None)|按住|value为按键，element为按键的元素，空为当前|
|key_up(value, element=None)|松开|value为松开的键，element为松开的键的元素，空为当前|
|move_by_offset(xoffset, yoffset)|移动当前|xoffset为x轴偏移，yoffset为y轴偏移|
|move_to_element(to_element)|移动到一个元素的中间|to_element为移动到的元素|
|move_to_element_with_offset(to_element, xoffset, yoffset)|移动到元素上|to_element为移动到的元素，xoffset为x轴偏移，yoffset为y轴偏移| 
|perform(actions)|执行一组动作|actions为动作|
|Release(on_element=None)|释放一个元素的鼠标按键|None为释放的元素，空为当前|
|send_keys(value)|输入|value为输入的值|
|send_keys_to_element(element, keys_to_send)|输入键到元素|element为输入的元素，keys_to_send为输入的键|
+ 等待事件：
  - 显式等待：由确定的事件触发
```
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

driver = webdriver.Firefox()
driber.get("http://www.xxx.com")
try:
  element = WebDriverWait(driver,10).until(
    EC.presence_of_element_located((By.ID,"xxx"))) 
    # WebDriverWait默认每500毫秒调用一次ExpectedCondition，直到返回True为止。
finally:
  driver.quit()

```
|预期条件|说明|
|:---|:---|
|title_is|标题是某内容|
|title_contains|标题包含某内容|
|presence_of_element_located|元素加载完成|
|visibility_of_element_located|元素可见，需输入locator|
|visibility_of|元素可见，定位到元素|
|presentness_of_element_located|判断至少有一个元素存在数中|
|text_to_be_present_in_element|文本在元素中可见|
|text_to_be_present_in_element_value|文本在元素值中可见|
|frame_to_be_available_and_switch_to_it|切换到frame|
|invisibility_of_element_located|元素不可见|
|element_to_be_clickable|元素可点击|
|staleness_of|元素不在DOM中|
|element_to_be_selected|元素可选择|
|element_located_to_be_selected|元素可选中|
|element_selection_state_to_be|元素选中状态为某状态|
|element_located_selection_state_to_be|元素选中状态为某状态，传入locator|
|alert_is_present|网页是否存在alert|
  - 预期事件，配合WebDriverWait，等待特定的事件发生，才继续执行。
  
  - 隐式等待：等待特定的时间，如果在这个时间内没有找到元素就报错

###### 将爬虫接入Selenium
+  scrapy松散的结构，使的每个部分都可被拓展，我们要使的爬虫支持JavaScript解释可写一个下载中间件,同样的需要写__del__()方法。
+  示例：
   - 思路：使用prepare_request()方法对请求进行处理，启用浏览器进行渲染，将结果构造HTMLResponse返回。
   - 实现：
      - 在__init__()中初始化PhantomJS,WebDriverWait.
      ```
      def __init__(self,timeout=None,service_args={})
      self.logger = logging.getLogger(__name__) #打开日志
      self.timeout = timeout #设置超时时间
      self.browser = webdriver.PhantomJS(service_args=service_args)
      self.browser.set_window_size(1400, 900) #设置浏览器大小
      self.browser.set_page_load_timeout(self.timeout) #设置浏览器加载超时时间    
      self.wait = WebDriverWait(self.browser, timeout) #设置超时时间
      ```
      - 关闭浏览器
      ```
      def __del__(self):
        self.browser.close()
      ```
##### Splash(不用)与**scrapy_splash**
+  Splash是一个轻量级的浏览器，它使用Lua脚本语言，可以用来做爬虫，Splash可以渲染页面，并返回HTML。
+  Splash的安装：（最简单是设置一个Docker虚拟机）
   - 安装Splash：splash运行与localhost:8050端口
      ```
      $ docker pull scrapinghub/splash
      $ docker run -p 8050:8050 -p 8050:8050 -p 8050:8050 scrapinghub/splash
      ```
+ Splash是通过HTTP API提供服务，以下是Splash提供的API：
  -  render.html：渲染页面，以HTML形式返回
  -  render.jsp：渲染页面，以JSON形式返回
  -  render.jpeg：渲染页面，以JPEG形式返回
  -  render.png：渲染页面，以PNG形式返回
  -  render.har：渲染页面，以HAR形式返回
+ splash只是提供了一个Web的渲染服务和selenium差不多，而scrapy-splash搭载了一系列的scrapy开发中间件。
  - 安装：pip install scrapy-splash ，
  - 配置服务器访问地址：SPLASH_URL = 'http://localhost:8050'
+ Splash的响应对象：
  -  SplashRequest: 继承自Request，用于在Splash中渲染页面。
  -  SplashResponse：scrapy-splash提供的响应对象。
  -  SplashJsonResponse：scrapy-splash提供的JSON响应对象。
  -  SplashTextResponse：scrapy-splash提供的文本响应对象。
+ SplashJsonResponse的特殊功能：
  -  response.data：返回JSON格式的数据。
  -  若配置了Splash会话处理，可使用response.cookiejar访问当前的Cookie。
  -  若请求中使用了Scrapy-Splash‘魔术响应’（默认），会在响应中自动设定以下属性。
    -  response.headers填充headers中的键。
    -  response.url设置成原始的请求URL。
    -  response.status设置返回响应的状态码。
    -  response.body响应正文
+ 常用技巧：
  - 异步加载：
    -  在Splash中渲染页面时，可以设置一个等待时间，这样就可以等待页面中的异步加载完成。
    ```
    def start_requests(self):
      splash_args = { 'wait' : 0.5,} //设置等待时间
        for url in self.start_urls:
            yield SplashRequest(url, self.parse, endpoint='render.html', args=splash_args)
    ```
  - 截屏与网页内容同时加载：
  - 向Lua脚本传递参数：
+ 将爬虫接入Splash
  - 采用蜘蛛中间件：
### scrapy数据储存与后处理
+ 图片下载与储存(ImagePipeline)
  - 格式转换
  - 避免重复下载
  - 自动生成缩略图并保存
  - 检测图片长宽，并过滤不满足的图片。
  - 避免多次下载多个项目共享的图片。
  - **配置文件可指定保存位置，图片有效期，和缩略图大小尺寸**
+ 

##  **多线程下载**：
1. 一个任务就是一个进程，一个进程中的子任务就是线程。Cup分配资源给进程，线程共享资源，真正在CPU上运行的是线程。
2. 普通下载：以wd创建并打开文件，以f.write(r.content)写入（r.content二进制数据流）
3. 用-thread创建进程：
-  调用_thread 模块中的start_new_thread（）函数来产生新线程。
其语法格式如下：thread.start_new_thread(function,arge[,kwarge])
参数含义如下。
function：线程运行函数。
args：传递给线程函数的参数，使用空的元组来调用函数表示不传递任何参数，必须是元组类型。
kwargs：可选参数。
-	Threading 类常用的方法
方法	说明
run()	线程的入口点
start()	启动线程，通过调用run（）方法启动一个线程
join([time])	堵塞进程直到线程执行完毕，参数time指定超时间（s），超过指定时间就不再堵塞进程
isAlive()	检查线程是否活动
getName()	返回线程名
setName()	设置线程名

start new threadO函数创建一个线程并运行指定函数，当函数返回时，线程自动结束
3.用Thread创建进程：
-  _thread 是低级模块；threading 是高级模块，其对thread模块进行了封装，为线程提供了更强大的高级支持。绝大多数情况下只需要使用threading高级模块，threading提供了守护进程功能。
threading 模块提供了Thread 类来创建和处理线程，其有两种使用方法：直接传入要运行的方法，或从Thread 继承并覆盖run()函数。
Thread类创建线程的语法格式如下：
线程对象=threading.Thread(target=线程函数，args=（参数列表），name=线程名，group=线程组）线程名和线程组都可以省略。
- Threading 模块提供的其他方法
其他方法	说明
threading.curremtThread()	返回当前线程的变量
threading.enumerate()	返回一个包含正在运行的线程目录
threading.activeCount()	返回正在运行的进程数
**锁与同步**当一个线程1要访问共享数据时必须获得锁定，如果有别的线性已经获得锁定了，线程1就暂停称为同步阻塞。多进程稳定占资源，多线程易崩溃。



# **反爬机制**
## 请求头限制
+	服务器通过识别keywor是否在Request Headers下的User-Agent中。 构造请求头的参数，伪装为浏览器访问。（request函数中有headers参数）
Eg：headers = {User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)',
'AppleWebKit/537.36 (KHTML, like Gecko)',
'Chrome/63.0.3239.84 Safari/537.36',//浏览器信息,
'Referer':'http://www.baidu.com',//请求的URL,
'Connection': 'keep-alive']//http请求特点,
novel_req = urlib.request. Request(novel_url, headers=headers),}

+ 请求头的各部分的含义
- Accept：text/html,image/*（浏览器可以接收的类型)
-  Accept-Charset：ISO-8859-1（浏览器可以接收的编码类型）
-   Accept-Encoding:gzip,compress（浏览器可以接收压缩编码类型）
-  Accept-Language：en-us,zh-cn（浏览器可以接收的语言和国家类型）。
-   Host：请求的主机地址和端口。
- If-Modifed-Since: Tue, 11 Jul 200018：23：51 GMT（某个页面的缓存时间）。
-  Referer：请求来自于哪个页面的URL。
- User-Agent: Mozilla/4.0(compatible, MSIE 5.5， Windows NT 5.0，浏览器相关信息）。
- Cookie：浏览器暂存服务器发送的信息。
- Connection: close(1.0)/Keep-Alive(1.1)（HTTP请求版本的特点）。
- Date:Tue,11 Jul 200018:23：51GMT（请求网站的时间）。</em>

+ 通过访问频率（过快或间隔时间相同）
    -  利用time.sleep(number)增设延时，numbe= roundom.randint(0,10)秒。
    - 利用代理ip(透明代理即知你用了代理IP又知道你的真实IP，普通代理（仅知用了代理IP），高匿代理)访问，建立代理ip池，response = requests.get(url,proxies=proxies)
##  cookie限制（要先登录）：
  - cookies
    - 登录后抓包找到cookies字符串，需要转换为字典形式才可使用。常用字典推导式。
  - cookies组成
     * 登录后抓包找到cookie字段，如访问频繁可建立一个cookie池，随机取用。
     * 用字典保存用户登录所需用户名和密码等，请求时添加到date参数中。
     * <em>Domain：域，表示当前Cookies属于哪个域或子域下面。
     * Path：表示Cookies的所属路径。
     * Expire Time/Max-Age：表示Cookies的有效期。
     * Secure：表示该Cookies只能用HTTPS传输。
     * Httponly：表示此Cookies 必须用HTTP或HTTPS传输。
     * HasKeys：通过该值指示Cookie是否含有子键，返回一个bool值。
     * Name：表示Cookie的名称。
     * Value：单个Cookie的值。
     * Values：单个Cookie所包含的键值对的集合</em>
## JavaScript反爬：
- 将重要信息放在JavaScript中浏览器会执行JavaScript代码使的信息正常展示，而爬虫不能做到。（如Ajax动态加载数据）
- 通过抓包分析接口数据，模拟请求来获取数据。
## 证书验证
  - 设置请求参数verify=Falae即可关闭证书验证，默认是True。
  - 默认开启状态下，verify=证书保存绝对路径。可设置使用的证书。

# 邮件发送
1.  smtplib和email模块发送，POP3接收和下载文件。 


# **scrapy框架**
+ 框架：减少了大量的重复性工作。
+ 所有的请求除非特殊设定都要经过中间件。
## 框架
+ [图示](https://img2018.cnblogs.com/blog/1730057/201912/1730057-20191215180753182-134919908.png)
+ 引擎：
  - 处理数据流，通过接收到的数据流控制事件的触发。
+ 调度器：
  - 过滤Request()后加入队列。
+ 下载器：
  -  请求出队交给引擎，引擎交给下载器，下载数据返回response交给引擎。
+ spider：
  - 处理引擎交付的response，返回item和Request()给引擎。
+ 管道：
  - 处理引擎交付的item，进行持久化存储。
## 创建项目
+ 
  - scrapy startproject project_name
  - cd project_name
  - scrapy genspider spider_name spider.com(指定spider名和IP种子)
+  scrapy startproject project_name
   - 命令执行后会创建多个文件
     * scrapy.cf//项目的根目录
     * settings.py// 配置文件
     * spider.py//爬虫 
## **items.py文件**，
    Item数据容器，属于接口类。
   - 使用class定义语法和Field对象声明，如下。
```
import Scarpy

class Product(scarpy.item):
  name = scarpy.Field()
  price = scarpy.Field()
  stock = scarpy.Field()
  last_update = scarpy.Field(serializer=str)
```
  其中的Field函数：是模块中的一个工具，用于定义类中的属性。当你使用 field 函数时，它允许你为字段指定默认值以及其他一些属性。未定义时自动设为默认值。
## spider.py蜘蛛文件
  - spider.py
```
import scrapy

class FirstSpider(scrapy.Spider):
    name = "spider_name"//设置蜘蛛名
    allowed_domains = ["spider.com"]//爬取域
    start_urls = ["https://spider.com"]//IP种子，可有多个。

    def parse(self, response)://解析response函数，多态
        pass
```
  - parse函数实现示例
```
  def parse(self,response):
    response = BeautfiulSoup(response,'lxml')
    for item in response.find_all('item'):
      feed_item = FeedItem()
      feed_item['title']=item.title//数据的拼接
      feed_item['link']=item.link
      yield feed_item
```


## pipelines.py管道

### 定义：
处理item对象，必须返回一个item或Dropitem异常，使用类进行定义，**接口实例如下**；
```
import openpyxl #为了使用exle保存文件，非管道必须
  class myPipeline(object):
    
    def __init__(self):#可定义多个函数
      wb = openpyx.Workbook()
      ws = wb.active
      ws.title = 'Top200'
      ws.append('tiele','评分'，'主题')

    def close_spider(self,spider):
      #关闭时自动调用，钩子方法。
        self.wb.save('电影数据.xlsx')

    def open_spider(self,spider):
      #启动时自动调用
      pass

    def process_item(self,item,spider):
     #process_item处理item
      return item
```
### 管道类型
 + 过滤性管道：raise DropItem(),丢弃item数据。
 + 加工管道：c = a+b
 + 储存管道：定义三大方法，初始化，打开，关闭。

## middlewares.py中间件(都是钩子)
### 钩子定义
+ 钩子函数(方法)<=>回调函数：不是我们去调用而是主函数需要时自动去调用。
### return None
+ 返回None时，放行请求，其他值则拦截请求。
### 下载中间件
```
class SpidernameDownloaderMiddleware:
  def from_carwler(cls,crawler):
```
+ RefererMiddleware(请求中间件)
  - 截取URL请求进行加工
  - 添加cookies认证(钩子方法)：
```
    def process_request(self, request, spider):
        # Called for each request that goes through the downloader
        # middleware.

        # Must either:
        # - return None: continue processing this request
        # - or return a Response object
        # - or return a Request object
        # - or raise IgnoreRequest: process_exception() methods of
        #   installed downloader middleware will be called
        request.cookies = cookies_dict
        return None
```

+ cookieMiddleware(cookie中间件)
  - 添加cookies认证(钩子方法)：
```
    def process_request(self, request, spider):
    # 每个请求都要经过中间件，在请求头中添加referer
        # Called for each request that goes through the downloader
        # middleware.

        # Must either:
        # - return None: continue processing this request
        # - or return a Response object
        # - or return a Request object
        # - or raise IgnoreRequest: process_exception() methods of
        #   installed downloader middleware will be called
        request.cookies = cookies_dict
        return None
```
- 配置：COOKIE_ENABLED = True //启用改变所有的cookie
  COOKIE_DEBUG = false //记录所有的cookie
  
+ DefaultHeadersMiddleware(默认请求头中间件)
  - 用于设定默认请求头。
  - eg:
```
DEFAULT_REQUEST_HEADERS = {Agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/533.36"}
```

+ DownloadertimeoutMiddleware(下载器中间件)
  - 指定request下载超时时间
+ HttpProxyMiddleware(代理中间件)
  - 设置代理
  - 在spider中启用http认证，设置spider的http_user, http_passwd属性。
```
from scrapy.contrib.spiders import CrawlSpider

class someIntraneSiteSpider:
  http_user = "username"
  http_passwd = "password"
  naem = 'intranet.example.com'
```
+ HttpCompressionMiddleware(压缩中间件)
  - 启用压缩:
    - HttpCompressionMiddleware.ENABLED = True

+ HttpCacheMiddleware(缓存中间件)
  - 启用缓存:
    - HttpCacheMiddleware.ENABLED = True
  - 设置缓存路径:
   -  HttpCacheMiddleware.CACHE_DIR = "httpcache"
  - 设置缓存过期时间:
   -  HttpCacheMiddleware.EXPIRATION_SECS = 0
  - 设置缓存最大容量:
   -  HttpCacheMiddleware.IGNORE_SCHEMES = ["file"]
  - 设置缓存策略:
   -  HttpCacheMiddleware.IGNORE_SCHEMES = ["file"]
   -  HttpCacheMiddleware.IGNORE_HTTP_CODES = [400, 401, 402, 403, 405, 406, 407, 408, 409]

+ ChunkedTransferMiddleware(分块传输中间件)
  - 启用分块传输
  
+ RedirectMiddleware(重定向中间件)
  - 启用重定向:
    - REDIRE_ENABLED = True // 默认启用
  - 设置重定向最大次数:
    - REDIRECT_MAX_TIMES = 20 // 默认20

+ MetaRefreshMiddleware(自动重定向中间件)
  - 启用自动重定向:METAREFRESH_ENABLED = True
  - 设置自动重定向的最大延迟：METAREFRESH_MAXDELAY = 100

+ RetryMiddleware(重试中间件)
  - 启用重试:RETRY_ENABLED = True
  - 设置重试次数:RETRY_TIMES = '3' // 默认3次
  - 设置重试http状态码:RETRY_HTTP_CODES = [500, 502, 503, 504, 400, 408]
  
+ RobotsTxtMiddleware(robots协议中间件)
 - 启用robots协议:ROBOTSTXT_OBEY = True

+ Downloaderstats(下载统计中间件)
  - 启用下载统计:DOWNLOAD_STATS = True

+ UserAgentMiddleware(UserAgent中间件)
  - 用于覆盖spider默认的User-Agent的中间件。
  - 要使spider使用新的User-Agent，需要设置USER_AGENT = "新的User-Agent"

+ AjaxCrawlMiddleware(Ajax爬取中间件)
  - 启用Ajax爬取:AJAXCRAWL_ENABLED = True
  - 设置Ajax爬取的优先级:AJAXCRAWL_RETRY_TIMES = 10
### 蜘蛛中间件
+  DepthMiddleware(深度中间件)
  - DEPTH_LIMIT = 3 # 允许爬取的最大深度
  - DEPTH_PRIORITY = 0 # 是否根据深度进行优先级排序
  - DEPTH_STATS = True # 是否收集爬取信息。
+ HttpErrorMiddleware(请求错误中间件)
 - 设置允许的HTTP状态码，默认允许200~300。
 - 若想允许错误的状态码，则需要设置handle_httpstatus_list=[允许的状态码]，允许所有的状态码则设置为[]。

+ OffsiteMiddleware(去重URL中间件)
  - 过滤掉非本网站的URL(即不在Spider中定义的域名)。
  - 若spider中定义了allowed_domains则过滤掉非本网站的URL。
 
+ UrlLengthMiddleware(URL长度中间件)
  - 过滤掉URL长度大于指定长度的URL，用URLLENGTH_LIMIT设置。
+ 
## setting.py配置文件
### 中间件及管道配置路径
+ 中间件名_类名：503
### spider与管道的对应 
+ 通过setting.py文件指定。
  - #ITEM_PIPELINES = {
    "liemr006.pipelines.Liemr006Pipeline": 300,
   }
  - **优先级**：300自定义数字当有多个管道时数字小优先级高，数字为none设置为禁用。
### 下载延时与延时是否随机
+ DOWNLOAD_DELAY = 1 #延时1秒
+ RANDOMIZE_DOWNLOAD_DELAY = True #随机延时
### 并发数
+ CONCURRENT_REQUESTS = 30 #同时下载的请求数
+ CONCURRENT_REQUESTS_PER_DOMAIN = 10 #每个域名的并发数
+ CONCURRENT_REQUESTS_PER_IP = 10 #每个IP的并发数
### 日志显示
+ LOG_LEVEL = 'ERROR' #只显示出错的部分
  - CRITICAL - 严重错误(critical)
  -  ERROR - 一般错误(regular errors)
  -  WARNING - 警告信息(warning messages)
  -  INFO - 一般信息(informational messages)
  -  DEBUG - 调试信息(debugging messages)
## scrapy的内置方法
### Request()
  - url（必需）：请求的URL。
  - callback（可选）：自选的回调函数，用于处理响应数据。当请求返回后，不提供则使用默认的回调函数来处理响应。
  - method（可选）：HTTP 请求方法，可以是GET，POST。 
  - headers（可选）：请求头信息，以字典形式提供，允许自定义请求头。
  - cookies（可选）：请求中使用的 cookies，以字典形式提供。
  - body（可选）：请求体数据，通常在 POST 请求中使用。如果要发送 POST 请求，可以将请求体数据传递给此参数。
  - meta（可选）：一个包含元数据的字典，可用于在请求之间传递数据。这对于在请求之间共享信息非常有用，如用户身份验证令牌、页面标识等。
  - encoding（可选）：响应内容的字符编码，用于将响应内容解码为字符串。默认是 'utf-8'。

### post表单处理
+ 用户登录：将信息写入字典，可用FormRequest.from_response()方法生成请求对象。
### FromRequest对象
  + 继承Request对象专用于处理表单请求
 - 实例属性：
  - headers：请求头信息，以字典形式提供，允许自定义请求头。
  - body：请求体数据，通常在 POST 请求中使用。如果要发送 POST 请求，可以将请求体数据传递给此参数。
  - **meta**：一个包含元数据的字典，可用于在上下两个请求之间传递数据。这对于在请求之间共享信息非常有用，如用户身份验证。
    ```
    def parse_pagel(self, response):
       item  = MyItem()
       item['main_url'] = response.url
       request = scrapy.Request("http://www.taobao.com",
                                callback=self.parse_page2)
       request.meta['item'] = item
       return request
    def parse_page2(self, response):
       item = response.meta['item']
       item['page2_url'] = response.url
       return item
    ```
    *  meta属性保留了一些特殊的键来对request进行拓展
    * dont_redirect：布尔值，如果为True，则不会自动重定向请求。默认值为False。
    * dont_retry：布尔值，如果为True，则不会自动重试此请求。默认值为False。
    * handle_httpstatus_list：一个整数列表，包含此请求允许的HTTP状态码。默认值为[200]。
    * cookies：一个包含cookie的dict，dict的key为cookie的name，value为cookie的value。
    * dont_merge_cookies：布尔值，如果为True，则不会自动合并此请求的cookie。默认值为False。
    * redirect_urls：一个包含重定向URL的列表。
    * bindaddress：一个字符串，指定IP地址为请求地址。 

### response对象
+ |属性|类型|说明|
  |:--|:--|:--|
  |url|str|响应的URL|
  |meta|dict|从request.meta中复制的meta对象信息|
  |headers|dict|响应头信息|
  |status|int|响应状态码|
  |flags|list|包含响应状态的标志列表|
  |request|Request|请求对象|

#### TextResponse对象(response的子类)
+ 在response对象的基础上增加编码属性，用于承载二进制数据。
+ |新增属性|类型|说明|
  |:--|:--|:--|
  |encoding|str|响应的编码|
  |selector|Selector|使用selector.xpath()或selector.css()方法返回的selector对象|
#### HtmlResponse对象(TextResponse的子类)
+ 新增了HTML解析器属性，用于解析HTML。
+ **使用：response=HtmlRequest(url=url,body=body)**
```
from scrapy.http import HtmlResponse
frmo scrapy.selector import Selector

response = HtmlResponse(url=url,body=body,encoding='utf-8')
data = response.xpath('//span/text()').extract()
print(data)
```
#### XmlResponse对象(TextResponse的子类)
+ 新增了XML解析器属性，用于解析XML。


### 内置方法
  - response.xpath(xpath): 返回一个包含匹配结果的list。
  - response.css(css): 返回一个包含匹配结果的list。
  - response.urljoin(url): 返回一个绝对URL。
  - copy：创建request的副本。
  - replase：替换request的属性。
  - 类方法：
    * from_request(response[,formname=None,formdata=None,formxpath=None,formnumber=0,clickdata=None,dont_click=False,css=None,url=None,cb_kwargs=None,encoding=None,callback=None,method='GET',headers=None,body=None,cookies=None,meta=None,pri])
      - response:Response:响应体用于读取表单域进行预填充。
      - fromname：string：指定实例使用的表单名称。
      - formdata：dict：指定输入表单域的值。
      - fromxpath：string：指定实例使用的XPath表达式。
      - formnumber：int：指定实例使用的表单域的编号。
      - clickdata：dict：指定点击链接的表单域。
      - **dont_click：bool：如果为True，提交表单的行为不会被模拟为‘单击’提交**。
## 爬网的建立
+ URL的拼接：url = response.urljoin(helf.extract())其中helf为从网页得到的URL挺换的片段。
+ yield Request(new_url = url, callback = my_parse(self, Reaponse)) 生成新的URL使用Request返回,**在callback中指定回调方法，从而实现目录爬URL,指定处理详情页的处理方法。**
## 启动scrapy
+ 进入项目目录。
+  scrapy crawl project_name
   - -o选项，输出到文件eg： scrapy crawl -o file.json

## 通用的蜘蛛
+ 特点：大量并行爬取大量网站的部分内容，逻辑简单先爬取储存后处理。
+ **无需重写parse方法**

### XMLFeedSpider(爬取符合XML文档的蜘蛛基类)
+ **基类**：无法直接使用，需要继承其子类才可使用。
+ XML、HTML、HTML5：HTML5大量使用且严格遵守XML规则。
+ 与普通蜘蛛的区别：
  - 必须**先声明iteratior(迭代器)的属性**，其中可选HTML、XML、**iternodes**，前两个需要先读完再分析效率低，一般使用iternades也是默认值，但是HTML对XML的容错率高。
  - 无需重写parse方法，而是重写parse_node方法。
+ 可重写的属性：
  - iterator —— 迭代器类型
    * HTML:使用Selector迭代器，需将所有的DOM载入内存，大量数据不适用。
    * XML:使用Selector迭代器，需将所有的DOM载入内存，大量数据不适用。
    * iternodes:高性能基于正则表达式的迭代器。
  - itertag —— 指定筛选的XML标签
  - namespaces —— 具有特殊命名空间的XML文档，可通过此元组指定
+ 可重写的方法
  - parse_node:对一个节点进行处理。
  - adapt_response:在分析response前修改response内容。
  - process_results:对蜘蛛返回处理结果进行最后的处理。

### CrawlSpider(多层次深度爬网蜘蛛蜘蛛)
+ 爬网协议文件：robot.txt有allow和deny两种规则，a标签的follow属性为"nofollow"时不允许深入，蜘蛛则会文明的停止爬取，为“true”时允许follow(跟随)。
+ 无需重写parse方法，重写parse_items。
#### 需要的类
+ Rule:制定爬网规则和回调方法。
+ LinkExtractor:从response中提取链接。

#### LinkExtractors链接提取器
+ 导入：from scarpy.linkextractors import Linkkextractor
+ 有extract_link公共方法，接收response返回scrapy.link.link对象数组。
+ 使用方法：实例化后传入rule中即可。

#### Rule
+ 导入:from scrapy.contrib.spiders inport Rule
+ 使用实例：用第一个参数指定linkextractor(链接提取器)，callback指定处理response函数。
  ```
  rules={
    Rule(LinkExtractor(allow=('*\.xml)),callback='pares_item),
  }
  ```
 - 可选参数：
   * cb_kwargs:传递给回调函数的参数的字典。
   * follow:布尔值指定提取到的链接是否跟进。当callback为None时默认为Ture反之为False。
   * process_links:过滤链接等的方法。
   * process_request:处理（过滤）请求的方法。

### CSVFeedSpider(爬取CSV文件蜘蛛)
+ 与普通蜘蛛的区别：
  - 属性：
    * delimiter:设置每段的分隔符。
    * quotechar:设置采用的引号。
    * headers:在CSV文件包含用来提取字段的名称列表。
  - 无需重写parse方法，而是重写parse_row方法提取数据。


### SitemapSpider(根据sitemap深度爬网)
+ 由于网站反爬机制，使的大部分网站没有sitemap文件而无法使用。

## scrapy的工程管理与服务器部署
+ 满足了爬虫的持续迭代需求，使用scrapyd管理工具。
+ scrapyd可通过任何可连接到服务器的PC来控制爬虫，可看做scrapy的web封装。**拥有版本控制功能由Restful API提供**


## 爬虫系统的测试和调试
### 测试：使用scrapy shell命令，可交互式调试爬虫。
+ scrapy shell "https://www.skj520.com/a/90673/90673153/index.html" #交互打开该网页。
+ 尝试运行以下Python代码来检查XPath选择器是否能够正确提取URL：response.xpath('/html/body/div[6]/dl/dd/a/@href').extract() #路径测试
+ **相当于一个交互式的Python环境**



### 调试：使用scrapy log命令，可查看爬虫日志。

# CMD命令
1. 设定打开即以管理员身份运行：搜索框输入cmd->打开文件位置->属性->高级属性(管理员)
2. ipconfig 查看本机IP等
+ 文件管理
 - **切换工作目录**：先切换磁盘目录(输入D:)，输入cd python
 - dir 查看目录文件
 - **md+文件夹名，创建文件夹**
 - move+文件名+路径名，移动文件。
 - copy+文件名+路径名，复制文件。
 - del+文件名，删除文件。
+ 进程管理
  - tasklist 查询当前进程
  - 强制关闭进程：tasklist /f/pid 进程号码
+ ping 网址
  - 长时间打开网页导致卡顿，可用ipconfig /flushdns
  - **tracert 网址**：查看挑转了几个网关，可定位交换机位置。
## Beautiful Soup
1. 解析HTML文件 soup = BeautifulSoup(filename,"lxml")(可使用lxml(快速，较高容错，需要C语言库),xml(速度快，需要C库，支持XML),html.parser(标准库，均衡),html5lib（容错最高，可生成HTML5，但运行慢扩展差）)
2. 查找标签： soup.head(获取head标签信息) title标签  //.body(主体).b(主体一个部分的标签) .getText(获取标签的值)
3. 精准查找：
  - soup.find_all('a',id="try")属性定位，有属性时可用。
  - soup.find_all('a',class-="efg",id="try")多属性一起定位，更精准。
  - soup.find_all('a',href == re.compile("aaa"))正则匹配
  1. CSS选择
  - 通过id查找：soup.select("#try3"）。
- 通过class 查找：soup.selcct(".efg"）。
-  通过属性查找：soup.select(“a[class="efg"]1）。
## 对文件的操作：

## 数据读写库
### CSV读取与写入
1. import csv
2. csvfile = open('new.csv','w',newline='')#nwelin=''表示每行不用空行隔开，存在new.csv则打开文件否则创建该文件。
3. writer=csv.writer(csvfile)//加载文件到CSV对象中
4. writer.writerow(['afds','afdasf','dsaf])//写入一行数据
5. data = {((‘法’，‘啊’，‘阿帆’)，
            (‘啊’，‘阿帆’，‘发大水发')
    }
    writer.writerows(data)
csvfile.close()
6. reder(*fp)以列表形式输出
   DictReader(*fp)以字典形式输出
### Word
1. pip install python-word
2. 数据写入
from docx import Document
from docx.shared import Inches
2.1. 创建对象 document = Document()
2.2. 添加标题，其中“0”代表标题类型，共有4种类型，具体可在Word的“开始”→“样
式”中查看，document.add_heading('Python 爬虫"，0)
2.3. 添加正文内容并设置部分内容格式
p= document.add paragraph('Python 爬虫开发-1')
2.4. 设置内容加粗 p.runs[0].bold = True
2.5. 添加内容并加粗 p.add_run('数据存储-').bold = True
2.6. 添加内容 p.add run('Word-')
2.7. 添加内容并设置字体斜体 p.add_run('存储实例。').italic = True
2.8. 添加正文，设置“样式”→“明显引用”
document.add_paragraph('样式'-'明显引用'，style='IntenseQuote')
2.9. 添加正文，设置“项目符号” document.add_paragraph(
  'porjet符号1',style='ListNumber'
)
2.10. 添加图片 document.add_picture('test.png',width=Inches(1.25))
2.11. 添加表格 table = document.add table(rows=1, cols=3)
hdr_cells = table.rows[0].cells
hdr_cells[0].text = 'Qty'hdr_cells[1].text = 'Id'hdr_cells[21.text ='Desc'for item in range(2):
row_cells = table.add _row().cells
row_cells[0].text = 'a'
row_cells[11.text = 'b"row_cells[2].text ='c"
2.12 保存文件
document.add page_break()
document.save('test.docx')
### Excel（2007版及以上的需要用openpyxl库）
1. pip install xlrd
2.  pip install xlwt
+ 新建一个Excel文件 wb=xlwt.Workbook()
+ 新建一个Sheet ws= wb.add_sheet('Python', cell_overwrite_ok=True)
定义字体对齐方式对象 alignment = xlwt.Alignment()
设置水平方向  HORZ GENERAL, HORZ_LEFT, HORZ CENTER, HORZ_RIGHT, HORZ_FILLED HORZ_JUSTIFIED, HORZ CENTER ACROSS SEL, HORZ DISTRIBUTED
alignment.horz = xlwt.Alignment.HORZ CENTER
设置垂直方向  VERT_TOP, VERT_CENTER, VERT_ BOTTOM, VERT_JUSTIFIED, VERT_DISTRIBUTED
alignment.vert = xlwt.Alignment. VERT_CENTER
定义格式对象
style = xlwt.XFStyle()
style.alignment = alignment
合并单元格write _merge（开始行，结束行，开始列，结束列，内容，格式）
ws.write merge(0， 0， 0，5, 'Python 网络爬虫'，style)
写入数据 wb.write（行，列，内容）
    for i in range(2, 7):
      for k in range(5):
          ws.write(i, k, i+k)# Excel 公式 xlwt.Formula
    ws.write(i, 5, xlwt.Formula('SUM(A'+str(i+1)+':E'+str
插入图片，insert_bitmap(img， x， y， x1， yl， scale_x=0.8, scale_y=1)
图片格式必须为bmp
×表示行数，y表示列数
x1表示相对原来位置向下偏移的像素#y1表示相对原来位置向右偏移的像素#Scale_x、scale_y缩放比例
ws.insert _bitmap('E:\\test.bmp',9, 1, 2, 2, scale_x=0.3, scale_Y=0.3)
3. 直接使用xlrd读取，处理麻烦可采用Xlsx2csv.
### xlsx2csv(将Excel文件转换为CSV文件处理)
+ 作用：
    1. 转换为CSV文件，后用CSV库处理。
+ 实例：
```
from xlsx2csv import Xlsx2csv
```
## Tkinter图形用户界面
1. import tkinter
win = tkinter.Tk ()#创建新窗口
win.title( 'Hellowe' )#窗口标题
win.geometry('600x300')#原始大小
win.minsize('200','100')#最小大小
win.maxsize('800','800')
win.mainloop()#显示窗口
1. tkinter组件：


# HTML(以html为根元素的文档树)
## 基础知识
html根标签：所有内容，都必须要写在该标签里面。
head是头部标签：用于导入外部的资源信息和描述网页自身信息
title标题标签，它里面的内容会在网页的浏览器的选项卡上显示
body主体标签，网页中能看到到所有内容，都要写在该标签里面
## 标签
+ meta标签提供关于HTML文档的元数据。元数据不会显示在页面上，但是对于机器是可读的。**为单闭和标签**。
  - eg：\<meta charset='utf-8' />编码声明。
  - eg： \<meta name="keywords" content="web前端" />页面关键词。
  - eg：\<meta name="description" content="Web前端" />网页描述
  - eg：\<meta http-equiv="refresh" content="30;url=" />content内的数字代表时间（秒），既多少时间后刷新。如果加url,则会重定向到指定网页
+ <link>标签用于导入外部资源，比如：网页选项卡图标，外部样式
+ h1~h6: 是网页的内容标题标签，h1最大，h6最小。标题标签的特点是：字体会加粗
+ p: 段落标签，用于描述一段内容 
+ hr: 是水平线标签，用于对网页中的内容进行分隔
+ br: 手动换行,在网页中无论打多少空格，或者换多少次行，默认都只是一个空格.
+ strong是加粗标签，em倾斜标签。
+ img是图片标签
  - **src属性设置图片的地址eg：\<img src="./img/bkhm.jpg">**
  - title属性设置鼠标悬停提示信息
  - alt属性设置图片的替代文字，当图片无法显示时，显示对应的文字
  - width属性设置图片宽度，height属性设置图片高度，如果只设置了其中一个属性，另一个属性会等比缩放 
+ a:链接标签，页面间链接，锚链接，功能性链接。
  - 1.页面间链接：href属性设置链接的地址，可以是本地地址，也可以是网络地址。target属性设置目标窗口打开的位置，属性值_self替换自身窗口，_blank是打开新窗口。
  - 2.锚链接：用于当前页面的跳转，从页面的某个区域，跳转到另一个区域通常需要两个a标签，一个a标签通过name属性设置锚标记，另一个a标签通过href属性跳转到对应的锚标记处
  注意：href属性值需要加一个#号
  - 3.功能性链接：利用超链接打开本地的应用
+ div是分区标签，是一个块级标签，通常用于网页的布局
  span是范围标签，是一个行级标签，通常用于突出显示段落中的部分内容
+ 标签分类：
所有的标签可以分为两类：块级标签和行级标签(内联标签)
块级标签：该元素独占一行(h2,p)
注意：通常情况下，行级标签要放在块级标签里面使用
注意：p标签不能嵌套p标签




# CSS(html的样式表)
+ 基本语言格式：容器元素在前，子元素在后。  
## 数据分类
+ 结构化数据：数据库文件
+ 半结构化数据：有基本结构的数据(HTML、JSON)
+ 非结构化数据：无任何结构的数据
## 路径
+ 绝对路径：协议名://主机名：端口/路径
  - 协议名(schema)：http、https、file(本地网页)
  - 主机名(host):域名、IP地址
  - 端口号(port不写为默认值):http默认80，HTTPS默认443
  - 路径(path)
## 标签
+ 关系(无绝对关系只有相对关系)：
  - 嵌套关系
  - 并列关系
## CSS选择器
### 常见元素与属性
+ 元素
  - 元素都有属性(attribute)和文本(text)
  - a元素 -> 网络链接元素
+ 属性
  - string字符属性，查看数据去除了标签。 
### 标签选择器：通过标签来选择
  - 格式：selector('str').get()选择第一个，getall选择所有
  - **与浏览器配合**：元素面板Ctrl+F打开搜索框，输入标签可查看所有符合的标签.(**注：浏览器的数据经过渲染可能与代码选择的结果不同**)
### 类选择器
  - 格式：css('.tep').getall()//提取所有有top属性的标签
### ID选择器(ID通常具有唯一性)
  - 格式: css('#ID').getall() 


# 数据的可视化


# 其他知识
1. API：程序编程接口。(别人写好的程序提供给你一个使用接口)
